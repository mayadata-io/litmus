---
- hosts: localhost
  connection: local
  gather_facts: False

  vars_files:
    - test_vars.yml

  tasks:
    - block:

        ## RECORD START-OF-TEST IN LITMUS RESULT CR

        - include_tasks: /utils/fcm/create_testname.yml

        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'SOT'
            chaostype: "{{ test_name }}"

        - name: Check if the cspc pool exists
          shell: kubectl get cspc -n {{ openebs_ns }} --no-headers -o custom-columns=:metadata.name
          args:
            executable: /bin/bash
          register: cspc_list
          failed_when: cspc_name not in cspc_list.stdout

        - name: Obtain the number of cspi from CSPC pool config
          shell: >
            kubectl get cspc {{ cspc_name }} -n {{ openebs_ns }}
            --no-headers -o yaml | grep 'poolConfig:' |wc -l
          args:
            executable: /bin/bash
          register: cspi_count

        - name: Check if the desired number of pool pods are running
          shell: >
            kubectl get pods -n {{ openebs_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_name }}
            --no-headers | wc -l
          args:
            executable: /bin/bash
          register: pod_count
          failed_when: cspi_count.stdout != pod_count.stdout

        - name: Obtain the list of pool pods
          shell: >
            kubectl get pods -n {{ openebs_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_name }}
            --no-headers -o custom-columns=:.metadata.name
          args:
            executable: /bin/bash
          register: pool_pods

        - name: Obtain the node list where the pools are scheduled
          shell: >
            kubectl get pods -n {{ openebs_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_name }}
            --no-headers -o custom-columns=:spec.nodeName
          args:
            executable: /bin/bash
          register: node_list

        - include_tasks: /chaoslib/kubectl/cordon_drain_node.yaml
          vars:
            action: "drain"
            app_ns: "{{ openebs_ns }}"
            app: "{{ pool_pods.stdout_lines[0] }}"

        - name: Check if the corresponding pool pod is evicted
          shell: >
            kubectl get pods -n {{ openebs_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_name }}
            --no-headers -o custom-columns=:status.phase | grep Running | wc -l
          args:
            executable: /bin/bash
          register: pool_count
          until: pool_count.stdout|int == (cspi_count.stdout|int-1)
          delay: 10
          retries: 30

        - name: Obtain the running cstor pool pods
          shell: >
            kubectl get pods -n {{ openebs_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_name }} -o json
            | jq -r '.items[] | select(.status.phase == "Running" or ([ .status.conditions[] | select(.type == "Ready" and .status == "False") ] | length ) == 1 )
            | .metadata.name'
          args:
            executable: /bin/bash
          register: cstor_pods

        - name: Obtain the node where one of the above pods is scheduled
          shell: >
            kubectl get pods -n {{ openebs_ns }} {{ cstor_pods.stdout_lines[0] }}
            --no-headers -o custom-columns=:spec.nodeName
          args:
            executable: /bin/bash
          register: node

        - name: Drain the above node
          shell: >
            kubectl drain {{ node.stdout }} --ignore-daemonsets
            --delete-local-data  --force
          args:
            executable: /bin/bash
          async: 150
          ignore_errors: yes

        - name: Confirm that the pod disruption budget is honored
          shell: >
            kubectl get pods -n {{ openebs_ns }} -l openebs.io/cstor-pool-cluster={{ cspc_name }}
            --no-headers -o custom-columns=:status.phase | grep Running|wc -l
          args:
            executable: /bin/bash
          register: pod_count
          until: pod_count.stdout|int == (cspi_count.stdout|int-1)
          delay: 10
          retries: 30

        - set_fact:
            flag: "Pass"

      rescue:
        - set_fact:
            flag: "Fail"

      always:

        ## RECORD END-OF-TEST IN LITMUS RESULT CR

        - name: Obtain the drained node list
          shell: >
            kubectl get nodes -o jsonpath="{range.items[?(@.spec.unschedulable)]}{.metadata.name}:{end}" 
            | tr ":" "\n"
          args:
            executable: /bin/bash
          register: drained_nodes

        - name: Uncordon the nodes
          shell: >
            kubectl uncordon {{ item }}
          args:
            executable: /bin/bash
          register: result
          until: "'uncordoned' in result.stdout"
          delay: 20
          retries: 12
          with_items: "{{ drained_nodes.stdout_lines }}"

        - include_tasks: /utils/fcm/update_litmus_result_resource.yml
          vars:
            status: 'EOT'
            chaostype: "{{ test_name }}"

